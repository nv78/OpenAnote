{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import (\n",
    "    LocalFileStore,\n",
    ")\n",
    "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_openai_json(data,filename):\n",
    "    \"\"\"\n",
    "    Converts a given dataset into a JSON Lines (JSONL) file suitable for OpenAI's GPT-3.5 turbo model.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame or similar data structure): Input data containing text and labels.\n",
    "\n",
    "    The function processes the input data row by row, constructing conversations for each row with a system message, user message, and an assistant message. It then writes the generated conversation data to a JSONL file.\n",
    " \n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store conversation data\n",
    "    message_list = []\n",
    "\n",
    "    # Iterate through the rows in the input data\n",
    "    for _, row in data.iterrows():\n",
    "        # Create a system message as an initial instruction\n",
    "        system_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  f\"You are a factual chatbot that answers questions about for giving text. You only answer with answers you find in the text, no outside information.\" \n",
    "        }\n",
    "\n",
    "        # Append the system message to the conversation\n",
    "        message_list.append({\"messages\": [system_message]})\n",
    "\n",
    "        # Create a user message based on the 'text' column from the data\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{row['question']} based on {row['evidence_text']}  \"\n",
    "        }\n",
    "\n",
    "        # Append the user message to the conversation\n",
    "        message_list[-1][\"messages\"].append(user_message)\n",
    "\n",
    "        # Create an assistant message based on the 'coarse_label' column from the data\n",
    "        assistant_message = {\n",
    "            \"role\": 'assistant',\n",
    "            \"content\": row['answer']\n",
    "        }\n",
    "\n",
    "        # Append the assistant message to the conversation\n",
    "        message_list[-1][\"messages\"].append(assistant_message)\n",
    "\n",
    "    # Write the conversation data to a JSON Lines (JSONL) file\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        for message in message_list:\n",
    "            # Serialize the conversation data to JSON and write it to the file\n",
    "            json.dump(message, json_file)\n",
    "            json_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from packaging import version\n",
    "\n",
    "required_version = version.parse(\"1.1.1\")\n",
    "current_version = version.parse(openai.__version__)\n",
    "\n",
    "if current_version < required_version:\n",
    "    raise ValueError(f\"Error: OpenAI version {openai.__version__}\"\n",
    "                     \" is less than the required version 1.1.1\")\n",
    "else:\n",
    "    print(\"OpenAI version is compatible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Now we can get to it\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"Enter openai key\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_id,num_label,pandas_df):\n",
    "    df = pandas_df.iloc[:num_label]\n",
    "    filename = f'ft_increment_{num_label}.jsonl'\n",
    "    text_to_openai_json(df, filename)\n",
    "    loader = client.files.create(file=open(filename, \"rb\"), purpose='fine-tune')\n",
    "    fine_tuning_job = client.fine_tuning.jobs.create(training_file=loader.id, model=\"gpt-3.5-turbo-1106\")\n",
    "    return fine_tuning_job.id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_fine_tuning(job_id):\n",
    "    while True:\n",
    "        response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "        print(response.fine_tuned_model)\n",
    "        #print(response[\"fine_tuned_model\"])\n",
    "        if response.fine_tuned_model:\n",
    "            print(response.fine_tuned_model)\n",
    "            return response.fine_tuned_model\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import FlareChain\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(embedding_model='text-embedding-ada-002',data_path='txt/'):\n",
    "    # Load all the transcripts stored in the data folder\n",
    "    loader = DirectoryLoader(data_path, glob=\"**/*.txt\", show_progress=True)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split the documents into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=50)\n",
    "    documents = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Initialize OpenAI Embeddings\n",
    "    openai_embedder = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "    # Cache the embeddings for faster loadup\n",
    "    cache_store = LocalFileStore(\"./cache/\")\n",
    "    cached_embedder = CacheBackedEmbeddings.from_bytes_store(openai_embedder, cache_store, namespace=\"sentence\")\n",
    "\n",
    "    # Create the vector db\n",
    "    db = FAISS.from_documents(documents, cached_embedder)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = create_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myllm = ChatOpenAI(temperature=0.30, model_name=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "flare = FlareChain.from_llm(\n",
    "    llm=myllm,\n",
    "    retriever=db.as_retriever(),\n",
    "    max_generation_len=700,\n",
    "    min_prob=0.15, \n",
    "    instruction=\"You are a factual chatbot that answers questions about 10-K documents. You only answer with answers you find in the text, no outside information.\"\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flare_rag_pred(data, model_id):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    myllm = ChatOpenAI(temperature=0.30, model_name=model_id)\n",
    "\n",
    "    flare = FlareChain.from_llm(\n",
    "        llm=myllm,\n",
    "        retriever=db.as_retriever(),\n",
    "        max_generation_len=700,\n",
    "        min_prob=0.15, \n",
    "        )\n",
    "\n",
    "\n",
    "    syntheses = []\n",
    "    for index, row in data.iterrows():\n",
    "        result = flare.run(row['question'])\n",
    "        print(index,result)\n",
    "        syntheses.append(result)\n",
    "\n",
    "\n",
    "    return syntheses\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [] \n",
    "num_labels = []\n",
    "result = [] \n",
    "\n",
    "count = 0 \n",
    "for i in range(5): \n",
    "    count += 10\n",
    "    ft_id = fine_tune_model(model_id=\"gpt-3.5-turbo-16k\", num_label=count, pandas_df= df)\n",
    "    if wait_for_fine_tuning(ft_id) is not None:\n",
    "        model_ids.append(wait_for_fine_tuning(ft_id))\n",
    "        syntheses = flare_rag_pred(data=df,model_id=wait_for_fine_tuning(ft_id))\n",
    "        result.append(syntheses)\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myllm = ChatOpenAI(temperature=0.30, model_name=model_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flare_rag_pred(data, model_id):\n",
    "  \n",
    "    myllm = ChatOpenAI(temperature=0.30, model_name=model_id)\n",
    "\n",
    "    flare = FlareChain.from_llm(\n",
    "        llm=myllm,\n",
    "        retriever=db.as_retriever(),\n",
    "        max_generation_len=700,\n",
    "        min_prob=0.15, \n",
    "        instruction=\"You are a factual chatbot that answers questions about 10-K documents. You only answer with answers you find in the text, no outside information.\"\n",
    "        )\n",
    "\n",
    "\n",
    "    syntheses = []\n",
    "    for index, row in data.iterrows():\n",
    "        result = flare.run(row['question'])\n",
    "        print(result)\n",
    "        syntheses.append(result)\n",
    "\n",
    "\n",
    "    return syntheses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheses = []\n",
    "for model_id in model_ids:\n",
    "    print(model_id)\n",
    "    result = flare_rag_pred(data=df,model_id=model_id)\n",
    "    syntheses.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({ 'syntheses' : syntheses[0], 'answer' : df['answer'] } ).to_csv('ft-flare-rag-10.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({ 'syntheses' : syntheses[1], 'answer' : df['answer'] } ).to_csv('ft-flare-rag-20.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({ 'syntheses' : syntheses[2], 'answer' : df['answer'] } ).to_csv('ft-flare-rag-30.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({ 'syntheses' : syntheses[3], 'answer' : df['answer'] } ).to_csv('ft-flare-rag-40.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({ 'syntheses' : syntheses[4], 'answer' : df['answer'] } ).to_csv('ft-flare-rag-50.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lotus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
